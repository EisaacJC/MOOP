{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdMySlToXzNHSYokfD2k/W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EisaacJC/MOOP/blob/main/FirstAttempt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "7fY94A4HBO4g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def buckin_n6(x):\n",
        "    #This works well, so no changes, i believe\n",
        "    x1, x2 = x\n",
        "    term1 = 100 * np.sqrt(np.abs(x2 - 0.01*x1**2))\n",
        "    term2 = 0.01 * np.abs(x1 + 10)\n",
        "    return term1 + term2\n",
        "def himmelblau(x):\n",
        "    x1, x2 = x\n",
        "    return (x1**2 + x2 - 11)**2 + (x1 + x2**2 - 7)**2\n",
        "\n",
        "\n",
        "def gradient(f, x, h):\n",
        "    #Naive implementation of gradient\n",
        "    grad = np.zeros_like(x)\n",
        "    for i in range(len(x)):\n",
        "        xh = x.copy()\n",
        "        xh[i] += h\n",
        "        fxh = f(xh)\n",
        "        xh[i] -= 2*h\n",
        "        fxh2 = f(xh)\n",
        "        grad[i] = (fxh - fxh2) / (2*h)\n",
        "    return grad\n",
        "\n",
        "def hessian(f, x, h):\n",
        "    H = np.zeros((len(x), len(x)))\n",
        "    for i in range(len(x)):\n",
        "        for j in range(i, len(x)):\n",
        "            xph = x.copy()\n",
        "            xph[i] += h\n",
        "            xmh = x.copy()\n",
        "            xmh[i] -= h\n",
        "            fxph = f(xph)\n",
        "            fxmh = f(xmh)\n",
        "            H[i,j] = (fxph - fxmh) / (2*h)\n",
        "            H[j,i] = H[i,j]\n",
        "    return H"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb = np.array([-15, -3])\n",
        "ub = np.array([-5, 3])"
      ],
      "metadata": {
        "id": "RUw4Ia13BUBR"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def momenta(p):\n",
        "    if p==\"p\":\n",
        "        return np.random.uniform(-0.1,0.1)\n",
        "def sqp(f, x0, maxiter=1000, tol=1e-4, tol_f=1e-6):\n",
        "    h=0.1\n",
        "    fvals = []\n",
        "    points = []\n",
        "    for i in range(maxiter):\n",
        "        x0 = np.clip(x0, lb, ub) #this is not necessary so delete it after\n",
        "        fx = f(x0) #evaluation of the function\n",
        "        dfx = gradient(f, x0,h) #calculate the numerical gradient\n",
        "        \"\"\"remember that h towards zero can be interpreted also as stabilization\n",
        "        or in some cases being lower capable of finding other minima\"\"\"\n",
        "        fvals.append(fx)#append current value of the function\n",
        "        points.append(x0.copy())#ok, so copy the current solution\n",
        "        Hfx = hessian(f, x0,h)#numerical evaluation of Hessian of f(x)\n",
        "        #Some tricks taken from stackoverflow to be less computational expensive\n",
        "        dx = -np.linalg.inv(Hfx) @ dfx\n",
        "        #ok, now we're only doing \\nabla{f(x)}*dr, i believe(?\n",
        "        grad_proj = np.dot(dfx, dx)\n",
        "        t = line_search(f, x0, dx, grad_proj)\n",
        "        x0 =x0+(t * dx)\n",
        "        #a kind of momenta operator but a naive implementation and also check\n",
        "        #if this stochastic implementation isnt bad\n",
        "        if i>0 and abs(fvals[i] - fvals[i-1]) / abs(fvals[i-1]) < tol:\n",
        "            #This can take out of our local minima but also worsen the\n",
        "            #objective function\n",
        "            #is it that ok?¡\n",
        "            if np.random.uniform()>0.5:\n",
        "                x0=x0+momenta(\"p\")+t*dx\n",
        "            else:\n",
        "                continue\n",
        "        #asegurando que tras la perturbación se encuentre dentro del espacio\n",
        "        #las soluciones tienden a irse a -4\n",
        "        x0 = np.clip(x0, lb, ub)\n",
        "        #solución usando fuerza bruta, restringiendola\n",
        "        #al espacio de búsqueda.\n",
        "\n",
        "        #print(\"Iter {}, x0 = {}, f(x0) = {}\".format(i, x0, fx))\n",
        "    return x0, points, fvals\n",
        "\n",
        "def line_search(f, x0, dx, grad_proj):\n",
        "    \"\"\"Just some code that i took from here:\n",
        "    https://stackoverflow.com/questions/52204231/implementing-backtracking-line-search-algorithm-for-unconstrained-optimization-p\"\"\"\n",
        "    tol_ls = 1e-6\n",
        "    a, b = 0, 1\n",
        "    phi = f(x0)\n",
        "    while True:\n",
        "        t = (a + b) / 2\n",
        "        fx = f(x0 + t * dx)\n",
        "        if fx < phi - grad_proj * t + 0.5 * t**2:\n",
        "            b = t\n",
        "        else:\n",
        "            a = t\n",
        "        if abs(b - a) < tol_ls:\n",
        "            return t\n",
        "\n",
        "x0 = np.asarray([np.random.uniform(-15,-5),np.random.uniform(-3,3)])\n",
        "xmin, points, vals = sqp(buckin_n6, x0)\n",
        "print(xmin)\n",
        "print(buckin_n6(xmin))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jLugHeOBtkc",
        "outputId": "3ca65df9-96fb-4b31-f194-13e021b7b3fb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-13.88454379  -3.99999905]\n",
            "243.50967752701726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lb = np.array([-5, -5])\n",
        "ub = np.array([-5, 5])\n",
        "x0 = np.array([0, 0])\n",
        "result = sqp(himmelblau, x0)"
      ],
      "metadata": {
        "id": "o9iMbYzdOKZS"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gjh0WCiGS7-Z",
        "outputId": "7cc7e0c7-a873-4d5e-adf6-c2242bc3397f"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.        , -0.34406365])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPl82-yaSt57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}